{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85e76806ac32e4bcb80bdd9cecbb6ef53201a10c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a10f925d93643ad180b8956f6348538c207df2a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9b27f913eef19b027e57febc5fbeed3d9fca8ea",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f834619eda2e8675d02db265c6917062907c11a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e3a7c80a665d307e025318dbe46fc779a0c26f4"
   },
   "source": [
    "As it can be seen, there are several missing data points. Specifically, in age and cabin. \n\nSince, we need to predict the 'Survived' column based on other columns, we will need as much data as possible. Steps to do this:\n\n1. See correlation between survived and other columns\n2. Combine train and test dataset\n3. Extract titles from names\n4. Fill in missing data\n    1. Age\n    2. Embarked\n    3. Fare\n5. Add is_alone variable\n6. Convert data to categorical variables\n7. Normalize data to be between 0 and 1\n8. Separate labels (y) from data\n9. Create training and dev sets\n10. Set up then neural networks to predict y\n11. Create submission file and submit\n12. Try Other Models - SVM and Random Forest\n13. Average All Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d70a0ecf8493daf56e498123b79162516738db22"
   },
   "source": [
    "## Step 1: See correlation between survived and other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0462a2deb683d6659cce35c21a2cb5eb19357498",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.columns\n",
    "m = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce0f0ee806af1b49b6aa26dd06fd5dcc15a0771c"
   },
   "source": [
    "Since PassengerId and name is unique to an individual, there is no relation that can be drawn with 'Survived'. \n\nHowever, Pclass, Sex, SibSp, Parch, and Embarked are categorical variables with distinct values that can impact 'Survived'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a28ccdd13e7c483235b91560d3f3aa66d950bb1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catCols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
    "j = 0\n",
    "for i in catCols:\n",
    "    plt.figure(j)\n",
    "    sns.barplot(x = i, y = 'Survived', data = train)\n",
    "    plt.show()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f3ae84313db584ce8f3e2b996d2a2947e1b34f6"
   },
   "source": [
    "We can use all these variables as they all seem to have an impact on the survival of the passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ce935481b3310b9a4ad04e5bfe08f0289bdf074"
   },
   "source": [
    "## Step 2: Combine train and test dataset\n\nWe will be transforming the training set to make it suitable for use in a machine learning model. Eg. convert string variables to categorical variables. The words 'male' and 'female' cannot be directly used for input. Instead they have to be replaced by 0 and 1 to feed to the model.\n\nThe same transformations also need to be applied on the test set because the trained model expects the input in this format. The best way to achieve this is to combine the two dataset and apply all transformations together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0216622a49039fb609f58d6de0a831fe7ca0c03",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6656e2e2ae355991693d3c0242d6b6dc9fb63bb8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfs = [train,test]\n",
    "data = pd.concat(dfs,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61d785270888b7f1a4495c8aec3ed41dd1806748",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efcb0e3fe20c544b935d878dea71c4fe58af2adb"
   },
   "source": [
    "## Step 3: Extract titles from names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb8712d654aae316d75bfa4e4ae6e007d822e9c9"
   },
   "source": [
    "The names begin with certain titles - Master/Miss for children and Mr. / Mrs. for adults. These can be extracted to create a new feature using RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f42f627df772c1f1675ab80b34bc2d8ed5e7c9e2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def getTitles(names):\n",
    "    titleRegex = re.compile(r',.\\w+\\.')    \n",
    "    title = []\n",
    "    for str in names:\n",
    "        titlePat = re.search(titleRegex,str)\n",
    "        if titlePat is None:\n",
    "            title.append(str)\n",
    "        else:\n",
    "            x = titlePat.group()\n",
    "            x = x[2:len(x)-1]\n",
    "            title.append(x)\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitles(data['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e2f70f6a732836110b2b983db87081ad8bf0859",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "46d8ecf7d2014593e58828fb3056e01ffa57a240",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def getCleanTitles(title):\n",
    "    for i in range(len(title)):\n",
    "        if title[i] in ['Don', 'Sir', 'Jonkheer']:\n",
    "            title[i] = 'Noble'\n",
    "        elif title[i] in ['Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards)', 'Lady', 'Dona']:\n",
    "            title[i] = 'Noble'\n",
    "        elif title[i] in ['Mlle', 'Ms']:\n",
    "            title[i] = 'Miss'\n",
    "        elif title[i] == 'Mme':\n",
    "            title[i] = 'Mrs'\n",
    "        elif title[i] in ['Capt', 'Col', 'Dr', 'Major', 'Rev']:\n",
    "            title[i] = 'Other'\n",
    "    return title\n",
    "\n",
    "data['Title'] = getCleanTitles(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "881f5a506caa391c178f72b3e040b8a7b439f8c7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6165380d6b7c8dc2cb71c7f16158cd135e09c55a"
   },
   "source": [
    "## Step 4: Fill in missing data\n\nThe more data we have available, the better the predictions can be. Hence, instead of discarding rows with data missing in some columns, we will try to fill it with some reasonable value so that the rest of the data in the row does not go to waste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1798985421ac9a3d865dad7c8aed7a055513bb52",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "636a033f3c539593a1b3c9a84a859cf7b13f2648"
   },
   "source": [
    "Since Cabin has several missing values, we will not include it in our list of features. However, we can try filling in missing values in Age, Embarked and Fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d1e50dcf830d81b729b23df5d0f3aa5d1750bb0"
   },
   "source": [
    "### A. Age\n\nAge is closely associated with Title, hence we will use the mean of ages for titles to replace missing values for rows with those titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "909865535e41d3a87d61ecd7c979a4d3bc9066b7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.groupby('Title').Age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0e86b935748f13f93fe72478f07bc5c2eceac4d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Age'].fillna(data.groupby('Title')['Age'].transform(\"mean\"), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2641fc166c8c703a88b5870682dec91aa7dd1913"
   },
   "source": [
    "### B. Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6701663c257b881efaeff13cc4d159f68bf086a6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.loc[pd.isnull(data['Embarked'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00803c2c81bc61d81336305d46db8ec9c16a6690"
   },
   "source": [
    "There are 2 missing values in Embarked which can be found online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03f15df57f7b0c9f18bb679f1c8768cbd8b9fb9d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.loc[61,'Embarked'] = 'S'\n",
    "data.loc[829,'Embarked'] = 'S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32903e54a7142855719e5a24c2a2df215c7d72f4"
   },
   "source": [
    "As per this source:\n\nhttps://www.encyclopedia-titanica.org/titanic-survivor/amelia-icard.html\n\nhttps://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eeacb7175f3424ee9d9ca8ba2637e57228bc1c30"
   },
   "source": [
    "### C. Fare\n\nFare still has one missing value. Fill it with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4409dd8a47de2b2a1a39eb9ab7f5cde7564a773d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['Fare'].fillna(data['Fare'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40596645775b74c754c1f97aa81218ccbae0564b"
   },
   "source": [
    "## Step 5: Add is_alone variable\n\nThe chances of survival go up for families. To find if someone is alone, we check if the sum of SibSp and Parch is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "862c0f42a04bde59239c2e84c75b8c468e373a9a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "is_alone = (data['Parch'] + data['SibSp'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45e155c0623ca23faa1e72d590452cb7cc00c38a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['is_alone'] = is_alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeeac3812cba6a8fc28c8bbf9e0294e7219b9d14",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = 'is_alone', y = 'Survived', data = data[:m])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6013a041d442ace73ff027383657f7086dca0df9"
   },
   "source": [
    "## Step 6: Convert data to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d8019d0483d55d29d56deeee5e0f852f804dfce",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab566c7033374a97a201671f611772227d691a25",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catCols.extend(['Title', 'is_alone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf8b0cbb6c11cd10f41004b0b24345ff6d499824",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3702d32891664d21b52958757312196dd142a165",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convertCatValToNum(catVal):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(catVal)\n",
    "    catVal = le.transform(catVal)\n",
    "    return catVal\n",
    "\n",
    "\n",
    "for i in catCols:\n",
    "    data[i] = convertCatValToNum(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00d0ff7e854e459705cd6cd4f64b1f9e8bcf1b6b"
   },
   "source": [
    "## Step 7: Normalize data to be between 0 and 1\n\nSince our output is binary i.e. either 0 or 1, training a machine learning algorithm is faster when inputs are also in the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a26f3c82abcfac85d102e6a341083293102c9d97",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "875ce963ac5ef4904754917424cc3d806281dec3"
   },
   "source": [
    "Out of the above columns we will select only those we think affect the outcome i.e. 'Survived' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb02168b4dcb1177fa38ae496cf6e2f8f44c0b4b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Xcols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'is_alone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "385d0d70ac83a12daa1e0c4df379fd253b41140a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[Xcols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b131bf07d526f2409c66ef4d6f65e24912faecf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data[Xcols])\n",
    "X = scaler.transform(data[Xcols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6cb949840c0733f68a6639dd0e1be7f3dc4deeb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check if the features have been correctly scaled\n",
    "\n",
    "X_stats = pd.DataFrame()\n",
    "X_stats['Min'] = np.min(X, axis = 0)\n",
    "X_stats['Max'] = np.max(X, axis = 0)\n",
    "X_stats['Mean'] = np.mean(X, axis = 0)\n",
    "X_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b256aabab97efe7fd56e2035c1b7a9ac643e8af"
   },
   "source": [
    "## Step 8: Separate labels (y) from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd497adfc6a95fdf27ac47cde037e8a2f9c0ac36",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = np.expand_dims(data[:m].Survived.values,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2376edd286ea4ee6a458c93a334e8c5840d0ef82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "549729ec26faa2a11501fce00c66e67b903a48e1"
   },
   "outputs": [],
   "source": [
    "# Save preprocessed data to file\n",
    "\n",
    "X_file = 'X.npy'\n",
    "#np.save(X_file, X)\n",
    "\n",
    "y_file = 'y.npy'\n",
    "#np.save(y_file, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "66073dabaff852912657e213dadcae439fb2ac33"
   },
   "outputs": [],
   "source": [
    "# Load preprocessed data from file\n",
    "\n",
    "\n",
    "#X = np.load(X_file)\n",
    "\n",
    "#y = np.load(y_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec4f998f4d6a76bb41844d46bd3607e784d9776a"
   },
   "source": [
    "## Step 9: Create training and dev sets\n\nValidation (dev) set is useful to know how the model performs on data it has not seen. We will randomly select 10% of the data as a validation set. The test set is already provided separately by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f3e79dd820fcb92d68f45da8a6265dca8c0f512",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "\n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Get random training index\n",
    "\n",
    "train_index = np.random.choice(m, round(m*0.9), replace=False)\n",
    "dev_index = np.array(list(set(range(m)) - set(train_index)))\n",
    "\n",
    "test_index = range(m, data.shape[0])\n",
    "# Make training and dev\n",
    "\n",
    "\n",
    "X_train = X[train_index]\n",
    "X_dev = X[dev_index]\n",
    "X_test = X[test_index]\n",
    "\n",
    "y_train = y[train_index]\n",
    "y_dev = y[dev_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c4a42e034485132f9e36a792bee30094eab4da0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83ecba1186ff8a3b3803092a12e261402d4176e8"
   },
   "source": [
    "## Step 10: Set up neural network to predict y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74391b16e58d8316db88a8d309c858681beefac1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize placeholders for data\n",
    "n = X.shape[1]\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, n], name = 'inputs_ph')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 1], name = 'labels_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "005713ce5c6cc97468437648bd34bc46b8ca22f8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# number of neurons in each layer\n",
    "\n",
    "input_num_units = n\n",
    "hidden_num_units = 120\n",
    "output_num_units = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "811074055a110506c412ba45b3fd52bc1a92bf26",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build Neural Network Weights\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "weights = {\n",
    "    'hidden1': tf.Variable(initializer([input_num_units, hidden_num_units])),\n",
    "    'hidden2': tf.Variable(initializer([hidden_num_units, hidden_num_units])),\n",
    "    'hidden3': tf.Variable(initializer([hidden_num_units, hidden_num_units])),\n",
    "    'output': tf.Variable(initializer([hidden_num_units, output_num_units])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden1': tf.Variable(initializer([hidden_num_units])),\n",
    "    'hidden2': tf.Variable(initializer([hidden_num_units])),\n",
    "    'hidden3': tf.Variable(initializer([hidden_num_units])),\n",
    "    'output': tf.Variable(initializer([output_num_units])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f64ae40cd6994d1c0a03388169f190017b02ecf4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build model \n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name = 'keep_prob')\n",
    "\n",
    "hidden_1_layer = tf.add(tf.matmul(x, weights['hidden1']), biases['hidden1'])\n",
    "hidden_1_layer = tf.nn.dropout(tf.nn.relu(hidden_1_layer),keep_prob = keep_prob)\n",
    "hidden_2_layer = tf.add(tf.matmul(hidden_1_layer, weights['hidden2']), biases['hidden2'])\n",
    "hidden_2_layer = tf.nn.dropout(tf.nn.relu(hidden_2_layer),keep_prob = keep_prob)\n",
    "hidden_3_layer = tf.add(tf.matmul(hidden_2_layer, weights['hidden2']), biases['hidden2'])\n",
    "hidden_3_layer = tf.nn.dropout(tf.nn.relu(hidden_3_layer),keep_prob = keep_prob)\n",
    "\n",
    "output_layer = tf.matmul(hidden_3_layer, weights['output']) + biases['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fac17a998fc8dcb7a411ef8e19c61416fc1da32",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "learning_rate = 3e-4\n",
    "epochs = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "519627084bee0b497cb14052ddbfb2b75952277f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set loss function and goal i.e. minimize loss\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "goal = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e240f266dd5d65b2426afe2c7375dcdd9fd82be",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.round(tf.nn.sigmoid(output_layer), name = 'prediction')\n",
    "correct = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\n",
    "recall = tf.metrics.recall(labels = y, predictions = prediction)\n",
    "accuracy = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e9ed48dde04e541503f69e3264e675c80346c34",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store loss and accuracy while training the model\n",
    "\n",
    "loss_trace = []\n",
    "train_acc = []\n",
    "dev_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcc1359490f694efccbb0cae2d06fff250180d9b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Start tensorflow session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "75592014795d68527103131ef540fec15d484cef"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    sess.run(goal, feed_dict={x: X_train, y: y_train, keep_prob: 0.5})\n",
    "\n",
    "    # calculate results for epoch\n",
    "\n",
    "    temp_loss = sess.run(loss, feed_dict={x: X_train, y: y_train, keep_prob: 1})\n",
    "    temp_train_acc = sess.run(accuracy, feed_dict={x: X_train, y: y_train, keep_prob: 1})\n",
    "    temp_dev_acc = sess.run(accuracy, feed_dict={x: X_dev, y: y_dev, keep_prob: 1})\n",
    "\n",
    "    # save results in a list\n",
    "\n",
    "    loss_trace.append(temp_loss)\n",
    "    train_acc.append(temp_train_acc)\n",
    "    dev_acc.append(temp_dev_acc)\n",
    "\n",
    "    # output\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print('epoch: {:4d} loss: {:5f} train_acc: {:5f} dev_acc: {:5f}'.format(epoch + 1, temp_loss, temp_train_acc, temp_dev_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3078023dd97cba907b2dbc676d9d27bc2b3571e"
   },
   "source": [
    "## Step 11: Create submission file and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c2ba148353d87b5bb6fe3c34a0477a5f2d6e935",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29b49049f4e4af076fa3c92dddec92897356a0ce",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_preds_nn = sess.run(prediction, feed_dict ={x: X_train, keep_prob: 1})\n",
    "y_dev_preds_nn = sess.run(prediction, feed_dict ={x: X_dev, keep_prob: 1})\n",
    "y_test_preds_nn = sess.run(prediction, feed_dict ={x: X_test, keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "725fa45a93a2d8f0a3d6a1e724cb8826570e4c50"
   },
   "outputs": [],
   "source": [
    "# Save model for future predictions\n",
    "\n",
    "inputs_dict = {'inputs_ph': x, 'labels_ph': y, 'keep_prob': keep_prob}\n",
    "outputs_dict = {'prediction': prediction}\n",
    "tf.saved_model.simple_save(sess, 'simple', inputs_dict, outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "417f4899ca3e0068e9e1e5c0a72ef85bd4b24d81",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "bc50c04297bea0c4bb79b227fc89f2f38e797ada"
   },
   "outputs": [],
   "source": [
    "# Restore saved model\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    # Restore saved values\n",
    "    print('\\nRestoring...')\n",
    "    tf.saved_model.loader.load(sess, [tag_constants.SERVING], 'simple')\n",
    "    print('Ok')\n",
    "    # Get restored placeholders\n",
    "    x = graph.get_tensor_by_name('inputs_ph:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    # Get restored model output\n",
    "    prediction = graph.get_tensor_by_name('prediction:0')\n",
    "\n",
    "    # Initialize restored dataset\n",
    "    y_train_preds_nn_saved = sess.run(prediction, feed_dict={x: X_train, keep_prob: 1})\n",
    "    y_test_preds_nn_saved = sess.run(prediction, feed_dict={x: X_test, keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "f84c2f53bd372de7216d9032ba563719b78332ae"
   },
   "outputs": [],
   "source": [
    "# Check if original and restored predictions are same\n",
    "\n",
    "confusion_matrix(y_train_preds_nn_saved, y_train_preds_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c28b6fc1a1e004b99d2597dc8da97d9f1739726c"
   },
   "source": [
    "Since the confusion matrix only has diagonal elements, the predictions match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e99ec2b234c59d468071c5919434c004f29ec910",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_recall(labels, preds):\n",
    "    tp = int(np.dot(labels.T,preds))\n",
    "    fn = int(np.dot(labels.T,1-preds))\n",
    "    recall = tp/(tp+fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8ce897304818a68be60d29a626878e7254fbcf6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recall_nn = get_recall(y_train, y_train_preds_nn)\n",
    "recall_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ffc44b0ac3828b0e33517bd652e9a29356a703c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_nn'] = y_test_preds_nn.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfe7bfff15d614212e8c55d13fccdd1ae4ab023e",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived_nn']].to_csv('submission_nn.csv', index = False, header = ['PassengerId', 'Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fc8985976861d80e5f218b91f023e55cce824533"
   },
   "source": [
    "## Step 12: Try Other Models - SVM and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "89e882c988bfd49530e114ee16bcd6f12abaf6f9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "svclassifier = SVC()\n",
    "svclassifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7be185afe06dcce5bfb0bb5a61ba8e688720691",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_preds_svm = np.expand_dims(svclassifier.predict(X_train),1)\n",
    "train_acc_svm = np.mean(y_train == y_train_preds_svm)\n",
    "print('Train Accuracy for SVM: {:5f}'.format(train_acc_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6749c3a2269e2da21dbfb469a15b9b40c6edaadf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_preds_svm = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bc3446f0064c08b62e511cba4415c70056f72c5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recall_svm = get_recall(y_train, y_train_preds_svm)\n",
    "recall_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4444a267978a394dfb70de14bcae9d8a18e0679",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_svm'] = y_test_preds_svm.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7587a1d14121622c625380dfb749d23e3257357d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived_svm']].to_csv('submission_svm.csv', index = False, header = ['PassengerId', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "621f9a5a1bc01435c106d6f9656ab4ec9f69d299",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rfclassifier = RandomForestClassifier(n_estimators = 100, max_features = 4)\n",
    "rfclassifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0026a4cb1e2738f3a4764ee65e87298d4aca1d35",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_preds_rf = np.expand_dims(rfclassifier.predict(X_train),1)\n",
    "train_acc_rf = np.mean(y_train == y_train_preds_rf)\n",
    "print('Train Accuracy for Random Forest: {:5f}'.format(train_acc_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c3da05c5ab96c6382f30d075bdfa78265304f83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_dev_preds_rf = np.expand_dims(rfclassifier.predict(X_dev),1)\n",
    "dev_acc_rf = np.mean(y_dev == y_dev_preds_rf)\n",
    "print('Dev Accuracy for Random Forest: {:5f}'.format(dev_acc_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfb1dd12d29edbb3a8e112f654841a8c93a9a77f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recall_rf = get_recall(y_train, y_train_preds_rf)\n",
    "recall_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fd61856f1a1ddb3fac12c6ef0a3a9803a615765",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_preds_rf = rfclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0234daaa7f5a4df14833e3b950410d4def5d17b0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_rf'] = y_test_preds_rf.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93df12714d49c0ec8160f69e7a8c496616a6002a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived_rf']].to_csv('submission_rf.csv', index = False, header = ['PassengerId', 'Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6944e7028130beb3241706047aff366162a9ecbe"
   },
   "source": [
    "# Step 13: Average All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "289f09c0b0f681b65ca101568263de9b0022eefb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbf269ce3220158388d7f8585d5fba8eea61ed8c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_nn_wtd'] = test['Survived_nn']*recall_nn\n",
    "test['Survived_svm_wtd'] = test['Survived_svm']*recall_svm\n",
    "test['Survived_rf_wtd'] = test['Survived_nn']*recall_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aed5273aa31773e34ec24ce3bf68f8488535db8f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_preds_avg = np.round(np.mean(test[['Survived_nn', 'Survived_svm','Survived_rf']],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e3e995160d1d0e897f11a3d2e9e227b752a1632",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_preds_wtd_avg = np.round(np.mean(test[['Survived_nn_wtd', 'Survived_svm_wtd','Survived_rf_wtd']],axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa3d3ee6c9c785f5ccf11874bb43fadbef48e9ee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_avg'] = y_test_preds_avg.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0152adcb61ccd4ce7c758f872bb307937ed80cc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['Survived_wtd_avg'] = y_test_preds_wtd_avg.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78b4f75934459f4a40e40586b414ebb3451a1484",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived_avg']].to_csv('submission_avg.csv', index = False, header = ['PassengerId', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00e7a26d4cf0a191e6321601521dc1a67689c67c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test[['PassengerId', 'Survived_wtd_avg']].to_csv('submission_wtd_avg.csv', index = False, header = ['PassengerId', 'Survived'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
